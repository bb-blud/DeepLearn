{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = './pickled/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "alpha = 0.001     # Global regularization rate\n",
    "num_steps = 10001  # Training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "def Graph1():\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Parameters\n",
    "        learning_rate = 0.01\n",
    "        batch_size = 128\n",
    "\n",
    "        # tf Graph Input\n",
    "        X = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "        y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "        # Set model weights\n",
    "        W = tf.Variable(tf.zeros([784, 10]))\n",
    "        b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "        # Construct model\n",
    "        pred = tf.nn.softmax(tf.matmul(X, W) + b) # Softmax\n",
    "\n",
    "        # Minimize error using cross entropy    \n",
    "        reg_loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1)) + alpha*tf.nn.l2_loss(W)\n",
    "        # Gradient Descent\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(reg_loss)\n",
    "\n",
    "        # Predictions\n",
    "        valid_reg_prediction = tf.nn.softmax(tf.matmul(valid_dataset, W) + b)\n",
    "        test_reg_prediction  = tf.nn.softmax(tf.matmul(test_dataset, W) + b)\n",
    "\n",
    "######################################## Run the Graph #########################################################\n",
    "    start = time()\n",
    "    with tf.Session(graph=graph) as session1:\n",
    "\n",
    "      tf.initialize_all_variables().run()\n",
    "      print(\"Initialized\")\n",
    "\n",
    "      for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size),  :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        # Prepare  dictionary\n",
    "        _, l, predictions = session1.run(\n",
    "          [optimizer, reg_loss, pred], feed_dict={X : batch_data,\n",
    "\t\t\t\t\t\t  y : batch_labels})\n",
    "        if (step % 500 == 0):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_reg_prediction.eval(), valid_labels))\n",
    "      print(\"Test accuracy: %.1f%%\" % accuracy(test_reg_prediction.eval(), test_labels))\n",
    "\n",
    "    print( time() - start )\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 8000: 0.588391\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 83.2%\n",
      "Test accuracy: 89.2%\n",
      "8.07359600067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 7500: 0.679468\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 7000: 0.753507\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 83.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 6500: 0.635417\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 6000: 0.578683\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 83.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 5500: 0.669042\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 83.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 5000: 0.830461\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 82.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 4500: 0.646472\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 83.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 4000: 0.729477\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 3500: 0.636356\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 3000: 0.666336\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 2500: 0.697774\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 2000: 0.709083\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1500: 0.801483\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 82.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1000: 0.759210\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 500: 0.880955\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 80.5%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.302585\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 51.3%\n"
     ]
    }
   ],
   "source": [
    "Graph1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [],
   "source": [
    "def Graph2():\n",
    "    batch_size  =  128\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "      ############################################\n",
    "      # Helper function: crunch through one layer\n",
    "      def crunch(data, param):\n",
    "      \t  w,b = param\n",
    "          hid_logits = tf.matmul(data, w) + b\n",
    "          ReLUed = tf.nn.relu(hid_logits)\n",
    "          return ReLUed\n",
    "      ###############\n",
    "\n",
    "      ######## Parameters #######\n",
    "      hidden_size = 1024\n",
    "      starter_learning_rate = 0.0005\n",
    "\n",
    "      ####### Input data ##########\n",
    "      # Runtime placeholders for training minibatches\n",
    "      X_train = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "      y_train = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "\n",
    "      # Probability switch for drop_out\n",
    "      keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "      # The dataset itself is a constant\n",
    "      tf_train_data = tf.constant(train_dataset)\n",
    "      tf_valid_data = tf.constant(valid_dataset)\n",
    "      tf_test_data  = tf.constant(test_dataset)\n",
    "\n",
    "      ####### Variables ###########\n",
    "      # decay rate step counter\n",
    "      global_step = tf.Variable(0, trainable=False)\n",
    "      \n",
    "      # hidden layers\n",
    "      W1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "      b1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "      \n",
    "      lW = tf.Variable(\n",
    "        tf.truncated_normal([hidden_size, hidden_size]))\n",
    "      lb = tf.Variable(tf.zeros([hidden_size]))\n",
    "      \n",
    "      # activation layer\n",
    "      W = tf.Variable(\n",
    "        tf.truncated_normal([hidden_size, num_labels]))\n",
    "      b = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "       \n",
    "      ####### Training computation #######\n",
    "\n",
    "      hid1 = tf.nn.relu  (\n",
    "      \t    tf.nn.dropout(\n",
    "\t\t\t tf.matmul(X_train, W1) + b1,\n",
    "\t\t\t keep_prob))\n",
    "\t\t\t \n",
    "      lhid = tf.nn.relu  (\n",
    "      \t    tf.nn.dropout(\n",
    "\t\t\t tf.matmul(hid1, lW) + lb,\n",
    "\t\t\t keep_prob))\n",
    "\n",
    "      logits = tf.matmul(lhid, W)  + b\n",
    "      \n",
    "      #\n",
    "      loss = tf.reduce_mean(\n",
    "             tf.nn.softmax_cross_entropy_with_logits(\n",
    "\t\t\t                            logits, y_train)) + alpha*tf.nn.l2_loss(W1) \\\n",
    "                                                                      + alpha*tf.nn.l2_loss(lW) \\\n",
    "                                                                      + alpha*tf.nn.l2_loss(W)  # Regularization\n",
    "      # Optimizer\n",
    "      learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "      \t\t\t\t\t\t 800, 0.75, staircase=True)\n",
    "      \n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "      \n",
    "      ###### Predictions for the training, validation, and test data  ######\n",
    "      \n",
    "      # current batch prediction\n",
    "      batch_pred = tf.nn.softmax(logits)\n",
    "      # valid and test\n",
    "      def v_t(vp, tp):\n",
    "          for param in [(W1,b1),(lW,lb)]:\n",
    "                vp = crunch(vp, param )\n",
    "                tp = crunch(tp, param )\n",
    "\t  return vp, tp\n",
    "\t  \n",
    "      valid_prediction = tf.nn.softmax(tf.matmul(v_t(tf_valid_data, tf_test_data)[0], W) + b)\n",
    "      test_prediction  = tf.nn.softmax(tf.matmul(v_t(tf_valid_data, tf_test_data)[1], W) + b)\n",
    "\n",
    "\n",
    "    ############################ Run the Graph ##############################\n",
    "    start = time()\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.initialize_all_variables().run()\n",
    "      print(\"Initialized\")\n",
    "      for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        # Feed  dictionary for computation\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, batch_pred], feed_dict={X_train : batch_data,\n",
    "                                                      y_train : batch_labels,\n",
    "                                                      keep_prob: 0.95})\n",
    "        if (step % 1000 == 0):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "      print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "    print( time() - start )\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 89.2%\n",
      "318.882736921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 83.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 10000: 1177.946899\n",
      "Minibatch accuracy: 77.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 83.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 9000: 1029.190430\n",
      "Minibatch accuracy: 81.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 83.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 8000: 1054.619629\n",
      "Minibatch accuracy: 83.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 82.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 7000: 1166.821777\n",
      "Minibatch accuracy: 74.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 82.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 6000: 1212.757935\n",
      "Minibatch accuracy: 79.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 82.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 5000: 1122.982666\n",
      "Minibatch accuracy: 78.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 82.5%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 4000: 1175.996582\n",
      "Minibatch accuracy: 77.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 82.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 3000: 1130.191162\n",
      "Minibatch accuracy: 83.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 81.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 2000: 1231.240356\n",
      "Minibatch accuracy: 81.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 80.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1000: 1352.035889\n",
      "Minibatch accuracy: 76.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 13.5%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 6381.747070\n",
      "Minibatch accuracy: 18.0%\n"
     ]
    }
   ],
   "source": [
    "Graph2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---\n",
    "On logistic regression, nn.l2_loss, actually reduced the test accuracy from 88.5% to 84%. The problem was I had not set the regularization rate.\n",
    "Still, in order to not lose accuracy it has to be set to at most 0.001 or smaller. In which case it just matches the performance of not having a\n",
    "regularization term at all. In the 1-hidden ReLUed network, test accuracy actually jumped from 88.5% to 92.7% after introducing regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---\n",
    "Probably not getting the intended results, all scores are lowered, mini-batch, validation and test. Though test accuracy is still pretty good at\n",
    "83.8% with just 500 batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training. \n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---\n",
    "At first, drop out seems to slightly decrease accuracy. The highest observed accuracy without dropout was 92.7%. With the following \"keep\"\n",
    "probabilities accuracies were, 0.45 -> 91.8%, 0.55 -> 92.3%, 0.85 -> 92.5%. So approaching 100% keep probability seemed to approach the highest\n",
    "value of 92.7%. However with a drop probability of 0.95 there was an observed accuracy of 93.2%. There might be a slight improvement with dropout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "From the docs decay rate is calculated as:\n",
    "\n",
    "decayed_learning_rate = learning_rate *\n",
    "                        decay_rate ^ (global_step / decay_steps)\n",
    " \n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ]
   },
   "source": [
    "  First attempts at adding a second hidden layer have not produced good results. Model did not learn at all with an initial learning rate of 0.5, \n",
    "once lowered to 0.005 loss seized to explode into nan values. Still after increasing the num_steps to 8001 from 3001 in an attempt to expose it \n",
    "to more data, test accuracy hovers at just 90% down from the 93.2% of a single hidden layer, ReLUed, lightly dropped (keep_prop=0.95) neural network.\n",
    "Increasing the steps to 8001 increased the logistic regressions accuracy from 88.5% to 89.2%, that is some evidence to its benefit. The layers are\n",
    "naively implemented. The larger first layer (2048 nodes) is not ReLUed but dropout is applied, the second smaller (1024 nodes) is ReLUed but with\n",
    "no dropout. My intuition was to use dropout on a larger layer with \"fresher\" input and applying non-linearity subsequently. What it all this seems\n",
    "to indicate, to me, was that increasing exposure to data by increasing the steps to 8001 was the only thing that had a postive effect. Indeed\n",
    "returning to single ReLU layer with the slight .95 drop and retaining the 8001 steps increased NN test accuracy to 94.2%.\n",
    "  The addition of a decaying learning rate so far only increases accuracy by 0.02%, with an exponential decay base of .85 and decay step every 800\n",
    "iterations. Adding a second identical hidden layer again results in a drop in accuracy. A reduced starter learning rate of 0.005 is necessary for NN\n",
    "to learn at all."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": {},
   "version": "0.3.2",
   "views": {}
  },
  "name": "Assig3.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
